## 🖥 알파고에서 활용된 머신러닝-강화학습, 딥러닝 알고리즘

알파고(AlphaGo)는 구글 딥마인드(DeepMind)에서 개발한 바둑 인공지능으로, 딥러닝과 강화학습 기술을 결합하여 세계 챔피언을 이긴 획기적인 프로그램입니다. 알파고의 알고리즘을 이해하기 위해, 딥러닝과 강화학습의 주요 구성 요소와 알고리즘의 작동 방식을 상세히 설명하겠습니다.

### 알파고의 핵심 알고리즘 구성 요소

1. **정책 네트워크 (Policy Network)**
2. **가치 네트워크 (Value Network)**
3. **몬테 카를로 트리 탐색 (Monte Carlo Tree Search, MCTS)**
4. **자기 대국 (Self-Play)**

### 1. 정책 네트워크 (Policy Network)

**정책 네트워크**는 주어진 바둑판 상태에서 가장 유망한 수를 예측하는 딥러닝 모델입니다. 이 네트워크는 다음과 같은 알고리즘으로 작동합니다:

#### 정책 네트워크 알고리즘

- **입력**: 바둑판의 상태를 표현하는 데이터(예: 현재 바둑판의 돌 배치, 턴 정보 등).
- **모델**: 딥 신경망(Convolutional Neural Network, CNN)을 사용하여 입력을 처리합니다.
- **출력**: 가능한 모든 수에 대한 확률 분포를 제공합니다. 이 확률 분포는 주어진 상태에서 각 수가 얼마나 유망한지를 나타냅니다.

**훈련**: 
- **데이터**: 인간 기사의 게임 데이터를 사용하여 정책 네트워크를 훈련합니다.
- **손실 함수**: 예측된 확률 분포와 실제 게임에서의 수를 비교하여 손실을 계산합니다. 이를 통해 네트워크를 최적화합니다.

### 2. 가치 네트워크 (Value Network)

**가치 네트워크**는 특정 바둑판 상태가 최종적으로 승리할 확률을 예측합니다. 이 네트워크는 다음과 같은 방식으로 작동합니다:

#### 가치 네트워크 알고리즘

- **입력**: 바둑판의 상태를 입력으로 받습니다.
- **모델**: 딥 신경망(CNN)을 사용하여 입력을 처리합니다.
- **출력**: 상태가 승리할 확률(0에서 1 사이의 값)을 출력합니다.

**훈련**:
- **데이터**: 자가 대국을 통해 얻은 게임 결과를 사용하여 가치 네트워크를 훈련합니다.
- **손실 함수**: 예측된 승리 확률과 실제 게임 결과(승리/패배)를 비교하여 손실을 계산합니다.

### 3. 몬테 카를로 트리 탐색 (Monte Carlo Tree Search, MCTS)

**MCTS**는 게임 트리를 탐색하여 최적의 수를 찾는 알고리즘입니다. 알파고에서는 정책 네트워크와 가치 네트워크의 예측을 사용하여 탐색을 가속화합니다.

#### MCTS 알고리즘

1. **선택 (Selection)**:
   - 현재 상태에서 가능한 모든 수를 시뮬레이션합니다.
   - `UCB1`(Upper Confidence Bound) 공식을 사용하여 노드를 선택합니다.

2. **확장 (Expansion)**:
   - 선택한 노드에서 새로운 자식 노드를 추가합니다.

3. **평가 (Simulation)**:
   - 확장된 노드에서 시뮬레이션을 수행하여 승리 확률을 평가합니다.
   - 이 과정에서 가치 네트워크를 사용하여 승리 확률을 예측합니다.

4. **백업 (Backpropagation)**:
   - 시뮬레이션 결과를 바탕으로 각 노드의 값을 업데이트합니다.
   - 정책 네트워크와 가치 네트워크의 예측을 반영하여 업데이트합니다.

### 4. 자기 대국 (Self-Play)

**자가 대국**은 알파고가 스스로 게임을 플레이하며 학습하는 방식입니다. 이 방법은 모델의 성능을 지속적으로 개선하는 데 중요한 역할을 합니다.

#### 자가 대국 알고리즘

1. **게임 플레이**:
   - 정책 네트워크와 MCTS를 사용하여 자가 대국을 진행합니다.
   - 각 수를 둘 때, 정책 네트워크가 제공하는 확률 분포와 MCTS가 제공하는 탐색 정보를 사용합니다.

2. **학습**:
   - 자가 대국에서 얻은 게임 결과를 바탕으로 정책 네트워크와 가치 네트워크를 업데이트합니다.
   - 자가 대국을 통해 모델이 스스로의 전략을 평가하고 개선합니다.

### 종합적으로 보면

- **딥러닝**: 알파고는 딥러닝을 사용하여 바둑판 상태를 분석하고, 예측을 수행합니다. 정책 네트워크와 가치 네트워크는 각각 수의 선택과 상태의 가치를 평가합니다.
- **강화학습**: 알파고는 강화학습을 통해 최적의 전략을 학습합니다. MCTS와 자가 대국을 통해 게임 전략을 개선하고, 최적의 수를 선택합니다.

알파고의 개발자들은 이와 같은 알고리즘을 통해 복잡한 바둑 게임에서 뛰어난 성능을 발휘하는 인공지능을 만들어냈습니다. 이 과정에서 강화학습과 딥러닝의 조합은 강력한 성과를 이루는 데 중요한 역할을 했습니다.

<br>
<br>
<br>
<br>
<br>

---

## 머신러닝의 분류 모델
10개
- 랜덤포레스트
- KNN
- 의사결정트리
- XGB
- GBM
- LGBM
- 
